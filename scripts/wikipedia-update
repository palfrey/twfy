#!/bin/bash
# Get new wikipedia titles database
# Only runs on live site
#set -x

cd /data/vhost/www.theyworkforyou.com/dumps
rm -f all-titles-in-ns0.gz
rm -f all-titles-in-ns0

DUMPDATE=`wget -q -O - http://download.wikimedia.org/backup-index.html | grep "enwiki/" | perl -pi.bak -e "s/.*(\d\d\d\d\d\d\d\d).*/\\\$1/;"`
#echo "Wikipedia dump date $DUMPDATE"
wget -q -O all-titles-in-ns0.gz http://download.wikimedia.org/enwiki/$DUMPDATE/enwiki-$DUMPDATE-all-titles-in-ns0.gz
gunzip all-titles-in-ns0.gz
echo "load data infile '/data/vhost/www.theyworkforyou.com/dumps/all-titles-in-ns0' ignore into table titles;" | mysql
cd /data/vhost/www.theyworkforyou.com/mysociety/twfy/scripts
cat wikipedia-exceptions | mysql

#Full database:
#http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2

